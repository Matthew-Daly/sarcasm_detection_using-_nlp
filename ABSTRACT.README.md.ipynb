{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sarcasm Using The Onion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Motivation\n",
    "\n",
    "As companies' reliance on tools such as sentiment analysis and chatbots increases, the ability to correctly discern the genuine attitude of the public also increases. Imagine the following interaction:\n",
    "\n",
    "Customer: 'You've been a big help! Thanks for nothing'\n",
    "\n",
    "Bot: 'Your welcome!'\n",
    "\n",
    "Or, this review:\n",
    "\n",
    "'Ah, Bulls. What fond, fond memories I have of my one and only visit to your \"establishment.\"' \n",
    "\n",
    "\n",
    "Misunderstanding the true meaning in either could lead to decreased customer satisfaction or a misinterpretation of the general sentiment towards the company or product by the public. \n",
    "\n",
    "While overt feelings of anger, frustration, or satisfaction are becoming easier for ML models to identify, sarcasm adds an extra level of complexity.  The goal of this project is to explore the viability of building models that can correctly classify sarcasm in text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Data for this project posed an interesting problem. We needed accurately labeled and similarly-formatted sets of text.  While there are data sets of user, self-reported, sarcastic posts from Reddit and Twitter, they have significant formating issues and would need a large amount of preprocessing.  \n",
    "\n",
    "While the long term goal of sarcasm detection is for such forums; for our initial trials, we were looking for data with higher label accuracy, and which was predominantly free of spelling and grammar errors.     \n",
    "\n",
    "We found the perfect one in the \"News Headlines Dataset for Sarcasm Detection', on Kaggle (https://rishabhmisra.github.io/publications/). We are forever grateful to Prahal Arora and Rishabh Misra for their work. \n",
    "\n",
    "This set is composed of headlines taken from mainstream news sources and The Onion, a satirical news website. We can be reasonably confident that the headlines from The Onion are sarcastic, while still following established headline formatting conventions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing And Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we experimented with many different protocols for text preprocessing, the final process was the following:\n",
    "\n",
    " - data cleaning\n",
    "  \n",
    " - feature extraction and tokenization of the text: we employed sequenced tokenization, as\n",
    "   maintaining the context of the words was vital to judging the writer's true meaning \n",
    "  \n",
    " - we then padded the sequences to have a uniformed length\n",
    "  \n",
    " - with our final models, we also used a pre-trained embedding mask, GloVe, which is an 'unsupervised\n",
    "   learning algorithm for obtaining vector representations for words'  \n",
    "   \n",
    " - for our Machine Learning Models, we created a W2Vecorizer class with GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Pipeline, we tested several Machine Learning Methods. Below are their final accuracy cross-validation scores: \n",
    "\n",
    "- Random Forest: 0.734\n",
    "- Support Vector Machine: 0.72\n",
    "- Logistic Regression: 0.70\n",
    "\n",
    "We also ran a stand-alone XGboost Classifier:\n",
    "\n",
    " - XGB: 0.734\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not overly impressed with these results, we ran a several Deep Learning Algorithms. They are below, with there accuracy:\n",
    "\n",
    " - LSTM: .85\n",
    " - GRU: .84\n",
    " - CNN: .80\n",
    " \n",
    "\n",
    "In addition to those,  also attempted many self-created sequential models. By far the best results we achieved, was with a Bidirectional/LSTM:\n",
    "\n",
    " - BiD/LSTM: .87\n",
    " \n",
    "While we tried multiple hyperparameter settings, but we were not able to improve on this result. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toward Production "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Bidirectional/LSTM model as our engine, we created a locally hosted Flask app that allows users to enter headlines 'from the wild' and get a percentage indicating the likelihood it is from The Onion or a mainstream news source. \n",
    "\n",
    "If you would like to try it out, in your terminal, go to the directory for this project and enter the command 'python app.py'.\n",
    "\n",
    "Then go to your browser and go to http://localhost:5000/, there will be a text box where you can enter a headline. Enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "While amusing in nature - the overarching goal of this project was to test the feasibility of building production models that could reasonably detect sarcasm within text. \n",
    "\n",
    "While taking into consideration the type of data used, and an underwhelming final accuracy rate of 89%, I think we have shown that it is possible with current Deep Learning methods. \n",
    "\n",
    "With that said, much more work would be needed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "More training. There are two versions of the headlines data set.  The next logical step is to train the current model with on the second set, to see if we can improve accuracy. \n",
    "\n",
    "Continuous training. It might be interesting to scrap headlines daily from both The Onion and news sources and update the training on a weekly or monthly basis. \n",
    "\n",
    "Take it to the real world. In the end, our goal is to gauge the sentiment of real people, not headlines. Once we have a reasonably accurate model, we should apply it to real-world situations. \n",
    "\n",
    "With that in mind, we will (as always) need new data. Paramount to the above is gathering accurate uses of sarcasm by regular people. The more realistic the data, the better the result.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
